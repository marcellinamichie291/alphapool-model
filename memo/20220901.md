
ideas

- https://note.com/j26/n/n64d9c37167a6



- http://www.cs.bme.hu/~oti/portfolio/icpproject/20110227/ch4.pdf
- https://math.bme.hu/alkmat/alkmat2013/eloadasok/gyorfi.pdf
- http://www.szit.bme.hu/~oti/portfolio/articles/tgyorfi.pdf
- http://www.cs.bme.hu/~oti/portfolio/articles/NN.pdf


ある半径以内の長さkの過去サンプルの将来リターンに対してlogリターンを最大化するように重みを決める
様々なkと半径に対して戦略を作り、SAAで混ぜる
nearest neighborの実験結果は前日逆張りを捉えているだけな気もする
knn regressorと似た性能になるのでは？ -> knn regressor試したがぱっとしない
つまり、様々なハイパーパラメータをSAAで混ぜるアイデアは使えるけど、nnは使えないかも？
nnで分布に対して重みを最適化するから、分散が考慮されるのは面白い
マッチするサンプルを直近n個にすると、今やってるmean variance portfolioに近くなるのか
nが無限なら、普通のmean variance portfolioは半径無限のケースになるから、
mean variance portfolioを含むのか


- UPをshortとレバレッジに対応させて実験してみる (crypto portfolio) DONE (ぱっとしない)
- nearest neighbor portfolioをやってみる DONE (ぱっとしない)
- ポートフォリオはmodelとしてもメタモデルとしても試したい

https://arxiv.org/abs/2111.09170

beta vs rank

- どちらが良いとは言えない
- 特徴量、目的変数それぞれで、beta/rank/両方がありえる

lgbmの調整

- デフォルトパラメータのほうが性能良いかも
- stdなどの特徴量を入れても良いかも (銘柄カテゴリ変数になってるのかも)

銘柄が多いほどアンサンブル効果が高くなる仮説

- ポジションが打ち消し合うケースが増えるのでは？
- モデル全体の損益は似ていても、銘柄ごとの損益はモデルごとにかなり違う (ほぼ同じ場合もある)
- 利益につながらない銘柄はノイズでしか無いから、その取引量は本来ゼロにできる
- コストが減る余地はかなりあるのでは？
- analyzerで検証したい
- 独立したモデルを作れているかは、各銘柄の損益を見るのが良いかも

analyzerの改良

- pfの打ち消しあったポジション量
- pfの打ち消しあったトレード量
- モデル間の相関 DONE
- モデル間の打ち消しあったポジション量 DONE
- モデル間の打ち消しあったトレード量 DONE

無次元特徴量だけにしてみた

feature neutralizationについて

- rsi, stoch, vol, symbol embed, othersくらいしか実質的にファクター無い
- rsiとstochの反転によって成績が下がってる気がする
- feature neutralization的な考えが必要

方法1: ファクターの変化(反転など)に対して特徴量を不変にする

- x^2やabs(x)などを使う
- ツリー系に対しては、順序を変えるような変換(x^2など)が意味がある
- 想定していない変化には対応できない
- 厳しすぎるかもしれない。実際は全てのファクターが同時に反転するケースは少ないかもなので
- 逆に全てのファクターが常にばらばらに反転するケースを想定するなら、これで良いかも

方法2: ファクターを均等に使う

- ridgeのalphaを大きくしたり、colsample_bytreeを下げるような方法
- ファクターが少ないと限界がある
- era boostも有効かも

方法3: data augmentation

- ファクターを変化させたdataを作る
- PCA -> シャッフル -> 元に戻す。などをすれば反転以外のケースにも対応できそう
- nnならデータは変えずにdropoutを使えるかも

方法4: ファクターを増やすと解釈

- x^2やabs(x)などで新しいファクターを増やす
- 本当に独立なファクターが増えているかは、相関などで確認
- ファクターが増えれば方法2が効く

ベストプラクティス

- 無次元特徴量をたくさん作る
- 銘柄間の処理 (beta, pca, rankなど)
- 特徴量を増やす (sqr, abs, mul)
- ファクターを均等に使うように学習 (colsample_bytree, ridge)
- これらの仮説はきれいに成り立つことは無いので、データやCVや色々試すことを重視する

特徴量の性質

- dimensionless
- bounded
- finite tail

試す1
skew of rsi
std of rsi -> 効いた気がする
sharpe of rsi
...

試す2
代表とするファクター(各tのstoch, rsiなど)との相関を見ながらチューニング

試す3
sqr, abs, mulをrank前にやってみる

特徴量ランダム生成は結構ばらつきがあるらしい
アンサンブルしたらどうなる？

銘柄特徴量を除くと性能が上がる
時間特徴量は？

- 除かないほうが性能良かった
- abs特徴量とかも無い方が性能良かった

betaの計算間違えている気がする。
間違えていても性能良いから良いけど
修正してみる

TODO

- cloud run quota
- 特徴量をfinite tailにする
- 短い特徴量がnanになる問題
- 特徴量の時間を調整してみる (長すぎでは？)
- horizonを調整してみる (48, 96を試す)
- 0910_beta_1dcnnを研究

data aug

- 20220910_beta_fix2をベースに調査
- symbol data aug (成績上がる。ただのノイズ付加やbaggingと比べないと、これの効果かわからない)
- mixup < bagging < symbol data aug < bagging + symbol data aug
- synth asset data aug(10, t=2), synth asset data aug(30) < bagging < synth asset data aug(10) < symbol data aug
- 結論、symbol data augは使っても良いかもしれない。学習に時間がかかるが

era boost関連

- era bagging
- weighting based on cv eval
- weighting based on train eval
- random weighting
- select best oof eval model (最良の成績が出るということは、最も簡単な期間を除いて学習しているということ)
- p.10 https://www.imse.iastate.edu/files/2018/06/PhamHieu-dissertation.pdf
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7015067/
- どれもぱっとしない

era bagging仮説

- 誤差を分布シフト方向と、分布内にわける
- era baggingをすると、分布シフト方向に多様なモデルを作れる
- アンサンブルすると分布シフトに対して強くなる

結論

- サンプルを選んでも汎化性能に寄与しづらい気がする
- 他のところを改善したほうが良さそう

バリエーションに効く方法

- y: beta, rank, ranker
- y horizon: 4, 24, 48, 96
- feature timescale
- model: lgbm(et), ridge, nn



アイデア

- calc_posでポジション作ってからbeta調整したら？ (コスト考慮)

特徴量アイデア

- シャープは低いが安定しているものがある気がする。それを使ったら？

アイデア

- ftx jp universeで試す
- stockをやる (saxobankのユニバース)
