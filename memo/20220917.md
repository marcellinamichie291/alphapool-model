
戦略整理

- mv portfolio
- 単一銘柄
- 2銘柄
- 複数銘柄

単一銘柄

- symbol: eth
- horizon: 4
- model: lgbm
- y: raw

2銘柄

- symbol: btc/eth
- horizon: 24
- model: lgbm
- y: beta

複数銘柄

- horizon: 12, 24, 48, 96
- model: ridge, lgbm, nn
- y: beta, rank, ranker

アイデア

- 過去のbfモデルやftx複数銘柄の特徴量を使ったらどうなる？
- minor銘柄(少し混ぜるくらいなら)
- doge戦略
- universalとの相関が低いモデルをforkして改善 (m-20220904-nn, m-20220907-beta-rank, m-20220910-rank)
- 最強を改善 (m-20220910-beta-fix2)
- momentum戦略 DONE

TODO

- bfモデルベース
- ftx複数銘柄ベース


アイデア

- 時刻を考慮しないでval lossを下げる解: 一般的なkaggleのテクニックとかで性能上げる
- ファクターを均等に混ぜる解: colsample_bytreeを下げたり、ridgeの正則化を強くしたり
- 安定したファクターを使う解: double sharpeとかで特徴量選択
- ファクターを均等に混ぜるだけだと、新しいファクターを作れない気がする
- 非線形に学習させた解を混ぜたほうが良い気がする

アンサンブル (see 20220918_eda)

- baggingしないとばらつきが多いので、bagging後でアンサンブル効果があるか調べる必要がある
- positionでアンサンブルするのが良さそう(別モデルとしてデプロイ)
- 理由は3つ
- 1: 性能の保証がある (リターンはモデル平均。分散はモデル平均以下になる)
- 2: 実験的にpositionでアンサンブルしたほうがsharpeが高いことが多い
- 3: positionと取引量が減る
- pos_typeは多様性あまり無いけど、rankとbetaを平均するとsharpeが少し上がるときがある
- pos_type sharpe: rank < beta, rank + beta, normalize(rank + beta) 
- pos_type = betaのほうが、リターンは大きいから、betaで良いかも
- lgbmとHistGradientBoostingRegressorは多様性作れるけど、HistGradientBoostingRegressorの性能が低いだけかも
- y_typeは多様性作れるけど性能差があるから本当に効いているか不明
- lgbmとlgbm_clfはy_typeほどではないけど多様性作れる
- lgbmとlgbm_etは少し多様性作れるけど、bagging数増やすのと大差無いかも
- y_type beta, beta_simpleは多様性作れる
- objective l2, l1はあまり多様性作れないし、l1で性能落ちる
- colsample_bytreeは多様性作れる
- feature_unionはあまり多様性作れない
- y_type rank, rank_weightは多様性少し作れる
- y_type rank_beta_weightはbetaと似ている。ほぼ同じ性能で、少し多様性作れる
- y_type rank_beta_mean_weightは性能低い
- y_transform uniformは性能低い

アンサンブル方針

- 1つのモデルでアンサンブルする必要は無い (universal portfolioに任せる。baggingなどは良い)
- pos_type, y_type, 非線形度合い(colsample_bytree)で多様性作れる
- 現状のポートフォリオで足りないモデルを手作りが良さそう (モデルが増えると管理が大変なので)

numerai TCについて

- https://zenn.dev/katsu1110/articles/b517135ff42235
- online learningとの関係は？
- SAAだと他の予測値の影響を重みの分母でしか受けない
- SAAと似ている気がする。分母が報酬に影響するようにしたのがポイントかな

dcor

- 計算が遅い
- 時刻依存性を捉えられそう

rankについて

- 銘柄数が変わると分布が変わってしまう (時刻dcorが大きくなる)
- 乱数で銘柄数に依存させなくする？
- ついでにtest time data aug?
- 銘柄数を考慮せずに乱数足したら性能上がった気がする
- 考慮して一様にしても性能上がるわけでは無さそう
- 普通にnormal乱数を足すのが良さそう

パフォーマンス

- df[features] += noiseが遅かった。df._dataを見て、全てfloat64にして一つのブロックになるようにしたら速くなった
- df.loc[:, features] += noiseは常に遅い
- 重い計算はjoblib.memoryを使うことにした。本番では使わないように環境変数で分岐する (see 20220919_rank)

アイデア

- 銘柄に重み付けしたら？BTCが儲かりやすいモデルとかのアンサンブル
